{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "\n",
    "import pickle\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/home/octo/Dropbox'+ '/SPY7Dec.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading csv file\n",
    "def get_csv_pd(path):\n",
    "    #spy_pd=pd.read_csv('C:\\\\Users\\Michal\\Dropbox\\IB_data\\SPY.csv',sep=' ',names=['askPrice','askSize','bidPrice','bidSize'],index_col=0,parse_dates=True)\n",
    "    #spy_pd=pd.read_csv(path+'\\SPY.csv',sep=',',names=['askPrice','askSize','bidPrice','bidSize'],index_col=0,parse_dates=True)\n",
    "    spy_pd=pd.read_csv(path,sep=',',dtype={'askPrice':np.float32,'askSize':np.float32,\n",
    "                                           'bidPrice':np.float32,'bidSize':np.float32},index_col=0,parse_dates=True)\n",
    "    #spy_pd = pd.read_csv(path, usecols=['askPrice','askSize','bidPrice','bidSize'], engine='python', skipfooter=3)\n",
    "    return spy_pd\n",
    "'''\n",
    "def get_csv_pd_notime(path):\n",
    "    #spy_pd=pd.read_csv('C:\\\\Users\\Michal\\Dropbox\\IB_data\\SPY.csv',sep=' ',names=['askPrice','askSize','bidPrice','bidSize'],index_col=0,parse_dates=True)\n",
    "    #spy_pd=pd.read_csv(path+'\\SPY.csv',sep=',',names=['askPrice','askSize','bidPrice','bidSize'],index_col=0,parse_dates=True)\n",
    "    spy_pd = pd.read_csv(path, usecols=['askPrice','askSize','bidPrice','bidSize'], engine='python', skipfooter=3)\n",
    "    return spy_pd\n",
    "'''\n",
    "\n",
    "def preprocessing(df):\n",
    "    df.bidPrice=df.loc[:,'bidPrice'].replace(to_replace=0, method='ffill')\n",
    "    df.bidSize=df.loc[:,'bidSize'].replace(to_replace=0, method='ffill')\n",
    "    df.askPrice=df.loc[:,'askPrice'].replace(to_replace=0, method='ffill')\n",
    "    df.askSize=df.loc[:,'askSize'].replace(to_replace=0, method='ffill')\n",
    "    df=df.dropna()\n",
    "    # to exclude 0\n",
    "    df=df[df['bidPrice']>df.bidPrice.mean()-df.bidPrice.std()]\n",
    "    df=df[df['askPrice']>df.askPrice.mean()-df.askPrice.std()]\n",
    "    df['mid']=(df.askPrice+df.bidPrice)/2\n",
    "    df['vwap']=((df.loc[:,'bidPrice']*df.loc[:,'bidSize'])+(df.loc[:,'askPrice']*df.loc[:,'askSize']))/(df.loc[:,'bidSize']+df.loc[:,'askSize'])\n",
    "    df['spread']=df.vwap-(df.askPrice+df.bidPrice)/2\n",
    "    df['v']=(df.askPrice+df.bidPrice)/2-((df.askPrice+df.bidPrice)/2).shift(60)\n",
    "    df['return']=(df.askPrice/df.bidPrice.shift(1))-1\n",
    "    df['sigma']=df.spread.rolling(60).std()\n",
    "    return df\n",
    "\n",
    "'''\n",
    "def normalise(df,window_length=60):\n",
    "    dfn=(df-df.rolling(window_length).min())/(df.rolling(window_length).max()-df.rolling(window_length).min())\n",
    "    return dfn\n",
    "\n",
    "def de_normalise(data,df,window_length=60):\n",
    "    dn=(df*(data.rolling(window_length).max()-data.rolling(window_length).min()))+data.rolling(window_length).min()\n",
    "    return dn\n",
    "\n",
    "def normalise_z(df,window_length=12):\n",
    "    dfn=(df-df.rolling(window_length).mean())/(df.rolling(window_length).std())\n",
    "    return dfn\n",
    "\n",
    "'''\n",
    "def normalise(df,window_length=60):\n",
    "    data=df[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    dfn=data/data.shift(60)\n",
    "    return dfn\n",
    "\n",
    "def de_normalise(dfn,window_length=60):\n",
    "    data=df[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    data=dfn*data.shift(60)\n",
    "    return data\n",
    "\n",
    "#https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# monkey patch around bug in ARIMA class\n",
    "def __getnewargs__(self):\n",
    "\treturn ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "\n",
    "def arima_processing(df):\n",
    "    #data=df[['vwap','mid']]\n",
    "    df=df.dropna()\n",
    "    df['Lvwap']=np.log(df.vwap)\n",
    "    df['Lmid']=np.log(df.mid)\n",
    "    df['LDvwap']=df.Lvwap-df.Lvwap.shift(60)\n",
    "    df['LDmid']=df.Lmid-df.Lmid.shift(60)\n",
    "    df=df.dropna()\n",
    "    return df  \n",
    "\n",
    "def ARIMA_saving(data):\n",
    "    data=data.dropna()\n",
    "    data1=data.LDvwap\n",
    "    data2=data.LDmid\n",
    "    \n",
    "    model_vwap = ARIMA(data1,order=(2,1,2))  # tested from ARIMA.ipynb\n",
    "    #predictions = model.fit(disp=0).predict()\n",
    "    predictions_vwap =model_vwap.fit(disp=0).fittedvalues\n",
    "    # save model\n",
    "    model_vwap.fit().save('vwap_arima.pkl')\n",
    "    vwap_arima=np.exp(predictions_vwap+data.Lvwap.shift(60))\n",
    "    \n",
    "    model_mid = ARIMA(data2,order=(2,1,2))  # tested from ARIMA.ipynb\n",
    "    #predictions = model.fit(disp=0).predict()\n",
    "    predictions_mid =model_mid.fit(disp=0).fittedvalues\n",
    "    # save model\n",
    "    model_mid.fit().save('mid_arima.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "data=get_csv_pd(filename)\n",
    "data=preprocessing(data)\n",
    "data=data.dropna()\n",
    "data=arima_processing(data)\n",
    "data=data.dropna().tail(10000)\n",
    "data= ARIMA_saving(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No need to save KM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression, sklearn, svm:SVR,linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=get_csv_pd(filename)\n",
    "data=preprocessing(data)\n",
    "#data=normalise(data)\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>askPrice</th>\n",
       "      <th>askSize</th>\n",
       "      <th>bidPrice</th>\n",
       "      <th>bidSize</th>\n",
       "      <th>mid</th>\n",
       "      <th>vwap</th>\n",
       "      <th>spread</th>\n",
       "      <th>v</th>\n",
       "      <th>return</th>\n",
       "      <th>sigma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:32.244925</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.829987</td>\n",
       "      <td>576.0</td>\n",
       "      <td>263.834991</td>\n",
       "      <td>263.830017</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:34.284669</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.829987</td>\n",
       "      <td>563.0</td>\n",
       "      <td>263.834991</td>\n",
       "      <td>263.830017</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:35.874931</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>563.0</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:37.499670</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>17.0</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>263.839966</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:39.356631</th>\n",
       "      <td>263.850006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>17.0</td>\n",
       "      <td>263.845001</td>\n",
       "      <td>263.840546</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.003652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              askPrice  askSize    bidPrice  bidSize  \\\n",
       "2017-12-08 03:00:32.244925  263.839996      1.0  263.829987    576.0   \n",
       "2017-12-08 03:00:34.284669  263.839996      1.0  263.829987    563.0   \n",
       "2017-12-08 03:00:35.874931  263.839996      1.0  263.839996    563.0   \n",
       "2017-12-08 03:00:37.499670  263.839996      1.0  263.839996     17.0   \n",
       "2017-12-08 03:00:39.356631  263.850006      1.0  263.839996     17.0   \n",
       "\n",
       "                                   mid        vwap    spread         v  \\\n",
       "2017-12-08 03:00:32.244925  263.834991  263.830017 -0.004974  0.019989   \n",
       "2017-12-08 03:00:34.284669  263.834991  263.830017 -0.004974  0.014984   \n",
       "2017-12-08 03:00:35.874931  263.839996  263.839996  0.000000  0.019989   \n",
       "2017-12-08 03:00:37.499670  263.839996  263.839966 -0.000031  0.014984   \n",
       "2017-12-08 03:00:39.356631  263.845001  263.840546 -0.004456  0.019989   \n",
       "\n",
       "                              return     sigma  \n",
       "2017-12-08 03:00:32.244925  0.000038  0.003702  \n",
       "2017-12-08 03:00:34.284669  0.000038  0.003723  \n",
       "2017-12-08 03:00:35.874931  0.000038  0.003723  \n",
       "2017-12-08 03:00:37.499670  0.000000  0.003723  \n",
       "2017-12-08 03:00:39.356631  0.000038  0.003652  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=data.tail(10000)\n",
    "X=df[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "y=df.mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr_model=regr.fit(X,y)\n",
    "# save the model to disk\n",
    "filename_rgr = 'rgr.sav'\n",
    "pickle.dump(regr_model, open(filename_rgr, 'wb'))\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.9) #kernel='linear' #kernel='poly'\n",
    "svr_model = svr_rbf.fit(X, y)\n",
    "# save the model to disk\n",
    "filename_svr = 'svr.sav'\n",
    "pickle.dump(svr_model, open(filename_svr, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification is based on the previous predictions, so need to build the dataframe df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Model is already saved from \"/Dropbox/DataScience/ARIMA_model_saving.ipynb\". Here loaded and added to \"df_ml\"\n",
    "def ARIMA_(data):\n",
    "    ### load model\n",
    "    data=data.dropna()\n",
    "    predictions_mid=ARIMA_mid(data.LDmid)\n",
    "    predictions_vwap=ARIMA_vwap(data.LDvwap) \n",
    "    vwap_arima=np.exp(predictions_vwap+data.Lvwap.shift(60))\n",
    "    mid_arima=np.exp(predictions_mid+data.Lmid.shift(60))\n",
    "    df_ml['arima']=data.mid+vwap_arima-mid_arima\n",
    "    \n",
    "def ARIMA_mid(data):\n",
    "    ### load model\n",
    "    mid_arima_loaded = ARIMAResults.load('mid_arima.pkl')\n",
    "    predictions_mid = mid_arima_loaded.predict()\n",
    "    return predictions_mid\n",
    "\n",
    "def ARIMA_vwap(data):\n",
    "    ### load model\n",
    "    vwap_arima_loaded = ARIMAResults.load('vwap_arima.pkl')\n",
    "    predictions_vwap = vwap_arima_loaded.predict()\n",
    "    return predictions_vwap\n",
    "\n",
    "#### KALMAN moving average\n",
    "\n",
    "##KF moving average\n",
    "#https://github.com/pykalman/pykalman\n",
    "\n",
    "# Import a Kalman filter and other useful libraries\n",
    "from pykalman import KalmanFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import poly1d\n",
    "\n",
    "def kalman_ma(data):\n",
    "    #x=data.mid\n",
    "    x=data.mid\n",
    "    # Construct a Kalman filter\n",
    "    kf = KalmanFilter(transition_matrices = [1],\n",
    "                  observation_matrices = [1],\n",
    "                  initial_state_mean = 248,\n",
    "                  initial_state_covariance = 1,\n",
    "                  observation_covariance=1,\n",
    "                  transition_covariance=.01)\n",
    "\n",
    "    # Use the observed values of the price to get a rolling mean\n",
    "    state_means, _ = kf.filter(x.values)\n",
    "    state_means = pd.Series(state_means.flatten(), index=x.index)\n",
    "    df_ml['km']=state_means\n",
    "\n",
    "### Linear Regression, sklearn, svm:SVR,linear_model\n",
    "import pickle\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "## loading model saved from /Dropbox/DataScience/REG_model_saving.ipynb\n",
    "filename_rgr = 'rgr.sav'\n",
    "filename_svr = 'svr.sav'\n",
    "# load the model from disk\n",
    "loaded_rgr_model = pickle.load(open(filename_rgr, 'rb'))\n",
    "loaded_svr_model = pickle.load(open(filename_svr, 'rb'))\n",
    "\n",
    "def strat_lr(data):\n",
    "    #no normalization\n",
    "    \n",
    "    data=data.dropna()\n",
    "    X=data[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    #y=df[['mid']]\n",
    "    predict_regr=loaded_rgr_model.predict(X)\n",
    "    predict_svr=loaded_svr_model.predict(X)\n",
    "    \n",
    "    df_ml['REG']=predict_regr\n",
    "    df_ml['SVR']=predict_svr\n",
    "    \n",
    "    ## strat_lr(data,dfn) and below needed for normalized data\n",
    "    #df_ml['predict_regr']=predict_regr\n",
    "    #df_ml['predict_svr']=predict_svr\n",
    "    #df_ml['REG']=de_normalise(data.mid,df.predict_regr)\n",
    "    #df_ml['SVR']=de_normalise(data.mid,df.predict_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml=pd.DataFrame()\n",
    "\n",
    "#creating the ml dataset\n",
    "data=get_csv_pd(filename)\n",
    "data=preprocessing(data)\n",
    "data=data.dropna()\n",
    "dfn=normalise(data)\n",
    "df_arima=arima_processing(data)\n",
    "### prediction for last 60 points\n",
    "data=data.dropna().tail(5000)\n",
    "dfn=dfn.dropna().tail(5000)\n",
    "df_arima=df_arima.dropna().tail(5000)\n",
    "\n",
    "df_ml['mid']=data.mid\n",
    "df_ml['vwap']=data.vwap\n",
    "\n",
    "\n",
    "ARIMA_(df_arima)\n",
    "kalman_ma(data)\n",
    "strat_lr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_ml=df_ml.dropna()\n",
    "data_cl=data.tail(len(df_ml))\n",
    "'''\n",
    "a= np.where(df_ml.mid>df_ml.km,1,0)\n",
    "b= np.where(df_ml.mid<df_ml.km,-1,0)\n",
    "c=np.where(df_ml.mid>df_ml.arima,1,0)\n",
    "d=np.where(df_ml.mid<df_ml.arima,-1,0)\n",
    "e=np.where(df_ml.mid>df_ml.REG,1,0)\n",
    "f=np.where(df_ml.mid<df_ml.REG,-1,0)\n",
    "g=np.where(df_ml.mid>df_ml.SVR,1,0)\n",
    "h=np.where(df_ml.mid<df_ml.SVR,-1,0)\n",
    "'''\n",
    "data_cl['U']=np.where(df_ml.mid>df_ml.vwap,1,0)\n",
    "data_cl['D']=np.where(df_ml.mid<df_ml.vwap,-1,0)\n",
    "data_cl=data_cl.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>vwap</th>\n",
       "      <th>arima</th>\n",
       "      <th>km</th>\n",
       "      <th>REG</th>\n",
       "      <th>SVR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:32.244925</th>\n",
       "      <td>263.834991</td>\n",
       "      <td>263.830017</td>\n",
       "      <td>263.829376</td>\n",
       "      <td>263.828279</td>\n",
       "      <td>263.835012</td>\n",
       "      <td>263.734688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:34.284669</th>\n",
       "      <td>263.834991</td>\n",
       "      <td>263.830017</td>\n",
       "      <td>263.832971</td>\n",
       "      <td>263.828917</td>\n",
       "      <td>263.835012</td>\n",
       "      <td>263.740069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:35.874931</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>263.839996</td>\n",
       "      <td>263.838402</td>\n",
       "      <td>263.829971</td>\n",
       "      <td>263.840017</td>\n",
       "      <td>263.740093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:37.499670</th>\n",
       "      <td>263.839996</td>\n",
       "      <td>263.839966</td>\n",
       "      <td>263.843578</td>\n",
       "      <td>263.830925</td>\n",
       "      <td>263.840017</td>\n",
       "      <td>263.743573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08 03:00:39.356631</th>\n",
       "      <td>263.845001</td>\n",
       "      <td>263.840546</td>\n",
       "      <td>263.846789</td>\n",
       "      <td>263.832264</td>\n",
       "      <td>263.845022</td>\n",
       "      <td>263.744920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mid        vwap       arima          km  \\\n",
       "2017-12-08 03:00:32.244925  263.834991  263.830017  263.829376  263.828279   \n",
       "2017-12-08 03:00:34.284669  263.834991  263.830017  263.832971  263.828917   \n",
       "2017-12-08 03:00:35.874931  263.839996  263.839996  263.838402  263.829971   \n",
       "2017-12-08 03:00:37.499670  263.839996  263.839966  263.843578  263.830925   \n",
       "2017-12-08 03:00:39.356631  263.845001  263.840546  263.846789  263.832264   \n",
       "\n",
       "                                   REG         SVR  \n",
       "2017-12-08 03:00:32.244925  263.835012  263.734688  \n",
       "2017-12-08 03:00:34.284669  263.835012  263.740069  \n",
       "2017-12-08 03:00:35.874931  263.840017  263.740093  \n",
       "2017-12-08 03:00:37.499670  263.840017  263.743573  \n",
       "2017-12-08 03:00:39.356631  263.845022  263.744920  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data_cl[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "y1=data_cl[['U']]\n",
    "y2=data_cl[['D']]\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "lm = linear_model.LogisticRegression(C=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octo/anaconda2/envs/carnd-term1/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "svm_model_up=svm.fit(X,y1)\n",
    "lm_model_up=lm.fit(X,y1)\n",
    "svm_model_dn=svm.fit(X,y2)\n",
    "lm_model_dn =lm.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename_svm_model_up = 'svm_model_up.sav'\n",
    "filename_lm_model_up = 'lm_model_up.sav'\n",
    "filename_svm_model_dn = 'svm_model_dn.sav'\n",
    "filename_lm_model_dn = 'lm_model_dn.sav'\n",
    "pickle.dump(svm_model_up, open(filename_svm_model_up, 'wb'))\n",
    "pickle.dump(lm_model_up, open(filename_lm_model_up, 'wb'))\n",
    "pickle.dump(svm_model_dn, open(filename_svm_model_dn, 'wb'))\n",
    "pickle.dump(lm_model_dn, open(filename_lm_model_dn , 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading classification for LSTM model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### loading classification model from /Dropbox/DataScience/ML_20Sep\n",
    "filename_svm_model_up = 'svm_model_up.sav'\n",
    "filename_lm_model_up = 'lm_model_up.sav'\n",
    "filename_svm_model_dn = 'svm_model_dn.sav'\n",
    "filename_lm_model_dn = 'lm_model_dn.sav'\n",
    "# load the model from disk\n",
    "loaded_svm_up_model = pickle.load(open(filename_svm_model_up, 'rb'))\n",
    "loaded_lm_up_model = pickle.load(open(filename_lm_model_up, 'rb'))\n",
    "loaded_svm_dn_model = pickle.load(open(filename_svm_model_dn, 'rb'))\n",
    "loaded_lm_dn_model = pickle.load(open(filename_lm_model_dn, 'rb'))\n",
    "\n",
    "def classification_up_dn(data):\n",
    "    X=data[['askPrice','askSize','bidPrice','bidSize','vwap','spread','v','return','sigma']]\n",
    "    y1=data.U\n",
    "    y2=data.D\n",
    "    \n",
    "    \n",
    "    predict_svm_up=loaded_svm_up_model.predict(X)\n",
    "    predict_lm_up=loaded_lm_up_model.predict(X)\n",
    "    predict_svm_dn=loaded_svm_dn_model.predict(X)\n",
    "    predict_lm_dn=loaded_lm_dn_model.predict(X)\n",
    "    \n",
    "    data['predict_svm_up']=predict_svm_up\n",
    "    data['predict_lm_up']=predict_lm_up\n",
    "    data['predict_svm_dn']=predict_svm_dn\n",
    "    data['predict_lm_dn']=predict_lm_dn\n",
    "    \n",
    "    data['predict_svm']=data.predict_svm_up+data.predict_svm_dn\n",
    "    data['predict_lm']=data.predict_lm_up+data.predict_lm_dn\n",
    "    \n",
    "    data['UD']=np.where(np.logical_and(data.predict_svm>0,data.predict_lm>0),1,np.where(np.logical_and(data.predict_svm<0,data.predict_lm<0),-1,0))  \n",
    "       \n",
    "    df_ml['UD']=data.UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification_up_dn(data_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>vwap</th>\n",
       "      <th>arima</th>\n",
       "      <th>km</th>\n",
       "      <th>REG</th>\n",
       "      <th>SVR</th>\n",
       "      <th>UD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-04 20:55:36.097306</th>\n",
       "      <td>247.320007</td>\n",
       "      <td>247.312500</td>\n",
       "      <td>247.326894</td>\n",
       "      <td>247.328544</td>\n",
       "      <td>247.320012</td>\n",
       "      <td>247.288138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04 20:55:37.144216</th>\n",
       "      <td>247.330002</td>\n",
       "      <td>247.318756</td>\n",
       "      <td>247.339441</td>\n",
       "      <td>247.328683</td>\n",
       "      <td>247.330006</td>\n",
       "      <td>247.288138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04 20:55:38.206749</th>\n",
       "      <td>247.330002</td>\n",
       "      <td>247.328201</td>\n",
       "      <td>247.345436</td>\n",
       "      <td>247.328808</td>\n",
       "      <td>247.330006</td>\n",
       "      <td>247.288138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04 20:55:39.269286</th>\n",
       "      <td>247.330002</td>\n",
       "      <td>247.328201</td>\n",
       "      <td>247.340588</td>\n",
       "      <td>247.328922</td>\n",
       "      <td>247.330006</td>\n",
       "      <td>247.288138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04 20:55:40.331817</th>\n",
       "      <td>247.330002</td>\n",
       "      <td>247.328201</td>\n",
       "      <td>247.346310</td>\n",
       "      <td>247.329025</td>\n",
       "      <td>247.330006</td>\n",
       "      <td>247.288138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mid        vwap       arima          km  \\\n",
       "2017-08-04 20:55:36.097306  247.320007  247.312500  247.326894  247.328544   \n",
       "2017-08-04 20:55:37.144216  247.330002  247.318756  247.339441  247.328683   \n",
       "2017-08-04 20:55:38.206749  247.330002  247.328201  247.345436  247.328808   \n",
       "2017-08-04 20:55:39.269286  247.330002  247.328201  247.340588  247.328922   \n",
       "2017-08-04 20:55:40.331817  247.330002  247.328201  247.346310  247.329025   \n",
       "\n",
       "                                   REG         SVR  UD  \n",
       "2017-08-04 20:55:36.097306  247.320012  247.288138   0  \n",
       "2017-08-04 20:55:37.144216  247.330006  247.288138   0  \n",
       "2017-08-04 20:55:38.206749  247.330006  247.288138   0  \n",
       "2017-08-04 20:55:39.269286  247.330006  247.288138   0  \n",
       "2017-08-04 20:55:40.331817  247.330006  247.288138   0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### LSTM\n",
    "\n",
    "#df.loc[:, cols].prod(axis=1)\n",
    "def lstm_processing(df):\n",
    "    df=df.dropna()\n",
    "    df_price=df[['mid','vwap','arima','km','REG','SVR']]\n",
    "    #normalization\n",
    "    dfn=normalise(df_price,12)\n",
    "    dfn['UD']=df.UD\n",
    "    return dfn\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        b = dataset[i:(i+look_back), 1]\n",
    "        c = dataset[i:(i+look_back), 2]\n",
    "        d = dataset[i:(i+look_back), 3]\n",
    "        e=  dataset[i:(i+look_back), 4]\n",
    "        f = dataset[i:(i+look_back), 5]\n",
    "        g=  dataset[i:(i+look_back), 6]\n",
    "        dataX.append(np.c_[b,c,d,e,f,g])\n",
    "        #dataX.append(b)\n",
    "        #dataX.append(c)\n",
    "        #dataX.append(d)\n",
    "        #dataX.append(e)\n",
    "        #dataX.concatenate((a,bT,cT,dT,eT),axis=1)\n",
    "        dataY.append(dataset[i + look_back,0])\n",
    "    return np.array(dataX), np.array(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another function to handle normalization. normalizing and adding UD is not done rather nl.log() only of the 6 columns.\n",
    "def lstm_processing(df):\n",
    "    df=df.dropna()\n",
    "    df_price=df[['mid','vwap','arima','km','REG','SVR']]\n",
    "    #normalization\n",
    "    dfn=np.log(df_price)\n",
    "    return dfn\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        b = dataset[i:(i+look_back), 1]\n",
    "        c = dataset[i:(i+look_back), 2]\n",
    "        d = dataset[i:(i+look_back), 3]\n",
    "        e=  dataset[i:(i+look_back), 4]\n",
    "        f = dataset[i:(i+look_back), 5]\n",
    "    \n",
    "        dataX.append(np.c_[b,c,d,e,f])\n",
    "        #dataX.append(b)\n",
    "        #dataX.append(c)\n",
    "        #dataX.append(d)\n",
    "        #dataX.append(e)\n",
    "        #dataX.concatenate((a,bT,cT,dT,eT),axis=1)\n",
    "        dataY.append(dataset[i + look_back,0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalization\n",
    "df_lstm=lstm_processing(df_ml)\n",
    "df_lstm=df_lstm.dropna()\n",
    "dataset=df_lstm.values\n",
    "dataset = dataset.astype('float32')\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "X_,Y_ = create_dataset(dataset,look_back)\n",
    "    \n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_ = numpy.reshape(X_, (X_.shape[0],X_.shape[1],X_.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1s - loss: 28.8755\n",
      "Epoch 2/50\n",
      "0s - loss: 23.1425\n",
      "Epoch 3/50\n",
      "1s - loss: 18.9896\n",
      "Epoch 4/50\n",
      "1s - loss: 15.3697\n",
      "Epoch 5/50\n",
      "1s - loss: 12.2387\n",
      "Epoch 6/50\n",
      "1s - loss: 9.5556\n",
      "Epoch 7/50\n",
      "1s - loss: 7.2847\n",
      "Epoch 8/50\n",
      "1s - loss: 5.3936\n",
      "Epoch 9/50\n",
      "1s - loss: 3.8522\n",
      "Epoch 10/50\n",
      "1s - loss: 2.6306\n",
      "Epoch 11/50\n",
      "1s - loss: 1.6975\n",
      "Epoch 12/50\n",
      "1s - loss: 1.0189\n",
      "Epoch 13/50\n",
      "1s - loss: 0.5568\n",
      "Epoch 14/50\n",
      "1s - loss: 0.2690\n",
      "Epoch 15/50\n",
      "1s - loss: 0.1104\n",
      "Epoch 16/50\n",
      "1s - loss: 0.0365\n",
      "Epoch 17/50\n",
      "1s - loss: 0.0091\n",
      "Epoch 18/50\n",
      "1s - loss: 0.0015\n",
      "Epoch 19/50\n",
      "1s - loss: 1.6366e-04\n",
      "Epoch 20/50\n",
      "1s - loss: 9.3355e-06\n",
      "Epoch 21/50\n",
      "1s - loss: 2.7005e-07\n",
      "Epoch 22/50\n",
      "1s - loss: 3.2517e-08\n",
      "Epoch 23/50\n",
      "1s - loss: 2.9866e-08\n",
      "Epoch 24/50\n",
      "1s - loss: 2.9819e-08\n",
      "Epoch 25/50\n",
      "1s - loss: 2.9771e-08\n",
      "Epoch 26/50\n",
      "1s - loss: 2.9775e-08\n",
      "Epoch 27/50\n",
      "1s - loss: 2.9805e-08\n",
      "Epoch 28/50\n",
      "1s - loss: 2.9800e-08\n",
      "Epoch 29/50\n",
      "1s - loss: 2.9809e-08\n",
      "Epoch 30/50\n",
      "1s - loss: 2.9828e-08\n",
      "Epoch 31/50\n",
      "1s - loss: 2.9860e-08\n",
      "Epoch 32/50\n",
      "1s - loss: 2.9813e-08\n",
      "Epoch 33/50\n",
      "1s - loss: 2.9872e-08\n",
      "Epoch 34/50\n",
      "1s - loss: 2.9917e-08\n",
      "Epoch 35/50\n",
      "1s - loss: 3.0050e-08\n",
      "Epoch 36/50\n",
      "1s - loss: 2.9951e-08\n",
      "Epoch 37/50\n",
      "1s - loss: 3.0194e-08\n",
      "Epoch 38/50\n",
      "1s - loss: 3.0446e-08\n",
      "Epoch 39/50\n",
      "1s - loss: 3.0458e-08\n",
      "Epoch 40/50\n",
      "1s - loss: 3.0473e-08\n",
      "Epoch 41/50\n",
      "1s - loss: 3.0672e-08\n",
      "Epoch 42/50\n",
      "1s - loss: 3.0904e-08\n",
      "Epoch 43/50\n",
      "1s - loss: 3.2025e-08\n",
      "Epoch 44/50\n",
      "1s - loss: 3.1720e-08\n",
      "Epoch 45/50\n",
      "1s - loss: 3.2731e-08\n",
      "Epoch 46/50\n",
      "1s - loss: 3.2512e-08\n",
      "Epoch 47/50\n",
      "1s - loss: 3.3936e-08\n",
      "Epoch 48/50\n",
      "1s - loss: 3.4977e-08\n",
      "Epoch 49/50\n",
      "1s - loss: 3.5999e-08\n",
      "Epoch 50/50\n",
      "1s - loss: 3.7006e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbe143366a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back,5)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_,Y_, epochs, batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"28sep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
